Command:        mpirun -np 4 -x UCX_LOG_LEVEL=error ./main network/gridgen/grid-1000-unif
Resources:      4 nodes (36 physical, 36 logical cores per node)
Memory:         187 GiB per node
Tasks:          4 processes
Machine:        gl3028.arc-ts.umich.edu
Start time:     Sun Jan 19 2020 16:11:41 (UTC-05)
Total time:     1 second
Full path:      /home/annabro/parallel-max-flow

Summary: main is MPI-bound in this configuration
Compute:                                      8.3% ||
MPI:                                         91.7% |========|
I/O:                                          0.0% |
This application run was MPI-bound. A breakdown of this time and advice for investigating further is in the MPI section below. 

CPU:
A breakdown of the 8.3% CPU time:
Scalar numeric ops:                           0.0% |
Vector numeric ops:                           0.0% |
Memory accesses:                            100.0% |=========|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 91.7% MPI time:
Time in collective calls:                    22.7% |=|
Time in point-to-point calls:                77.3% |=======|
Effective process collective rate:            1.23 kB/s
Effective process point-to-point rate:         569 kB/s
Most of the time is spent in point-to-point calls with a very low transfer rate. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.

I/O:
A breakdown of the 0.0% I/O time:
Time in reads:                                0.0% |
Time in writes:                               0.0% |
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                 0.00 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!

Threads:
A breakdown of how multiple threads were used:
Computation:                                 92.3% |========|
Synchronization:                              7.7% ||
Physical core utilization:                    2.8% ||
System load:                                 53.8% |====|
Physical core utilization is low. Try increasing the number of threads or processes to improve performance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                     141 MiB
Peak process memory usage:                     144 MiB
Peak node memory usage:                      24.0% |=|
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how the 0.0118 Wh was used:
CPU:                                        100.0% |=========|
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
The whole system energy has been calculated using the CPU energy usage.
System power metrics: No Arm IPMI Energy Agent config file found in /var/spool/ipmi-energy-agent. Did you start the Arm IPMI Energy Agent?

